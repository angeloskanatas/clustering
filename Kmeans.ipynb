{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOy+yXe5Cn3vKKxIa9qILv/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Here we implement the K-means algorithm\n","class KMeans:\n","  \"\"\"\n","  Simple K-means clustering algorithm implementation.\n","    \n","  Parameters\n","  ----------\n","  n_clusters : int, default=2\n","      The number of clusters to form.\n","  max_iter : int, default=300\n","      The maximum number of iterations to run the algorithm.\n","  tol: float, default=1e-4\n","      Relative tolerance with regards to Frobenius norm of the difference\n","      in the cluster centers of two consecutive iterations to declare\n","      convergence.\n","  random_state : int, default=None\n","      Determines random number generation for centroid initialization. Use\n","      an int to make the randomness deterministic.\n","        \n","  Attributes\n","  ----------\n","  labels_ : ndarray of shape (n_samples,)\n","      The predicted labels of data points.\n","   n_iter_: int\n","      Number of iterations run.\n","  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n","      The coordinates of the final cluster centers.\n","  distance_: ndarray of shape (n_samples, n_clusters)\n","      The final squared distances between data points and cluster centers.\n","  inertia_ : float\n","      The sum of squared distances of the samples to their closest centroid.\n","  \"\"\"\n","\n","  def __init__(self, n_clusters=2, max_iter=300, tol=1e-4, random_state=None):\n","    self.n_clusters = n_clusters\n","    self.max_iter = max_iter\n","    self.tol = tol\n","    self.random_state = random_state\n","    \n","  def fit(self, X):\n","    # Initialize centroids randomly (we choose randomly n_clusters data points from our dataset)\n","    # We can initialize the centroids with numerous other ways (e.g. the initialization in 'k-means++' algorithm) (fixme: implement this and compare the results)\n","    # For better results we can run the algorithm multiple times with different random initializations and choose the one with the best objective value (fixme: implement this and compare the results)\n","    # Changes for time efficiency can also be done\n","    rng = np.random.default_rng(self.random_state)\n","    self.cluster_centers_ = X[rng.permutation(X.shape[0])[:self.n_clusters]]\n","    self.distance_ = np.zeros((X.shape[0], self.n_clusters))\n","        \n","    # Iterate until convergence or maximum number of iterations is reached\n","    for i in range(1, self.max_iter+1):\n","      # Calculate squared Euclidean distance between data points and cluster centers (fixme: we can use other norms as well and compare the results)\n","      for j in range(self.n_clusters):\n","        self.distance_[:, j] = np.sum((X - self.cluster_centers_[j]) ** 2, axis=1)\n","\n","      # Classify data points to the nearest centroid\n","      self.labels_ = np.argmin(self.distance_, axis=1) # predicted labels\n","            \n","      # Compute new centroids as the mean of the assigned data points in each cluster\n","      new_centroids = np.array([X[self.labels_ == label].mean(axis=0) for label in range(self.n_clusters)])\n","\n","      # Count the iterations of the algorithm so far\n","      self.n_iter_ = i\n","\n","      # Calculate the objective function/inertia\n","      self.inertia_ = np.sum(self.distance_[np.arange(len(self.distance_)), self.labels_])\n","            \n","      # Check for convergence (with regards to Frobenius norm - we can use numerous other ways)\n","      error = np.linalg.norm(self.cluster_centers_ - new_centroids)\n","      if error < self.tol:\n","        break\n","      if i != self.max_iter: # do not update the centroids in last iteration\n","        self.cluster_centers_ = new_centroids # update the centroids if not converged yet\n","  \n","  def predict(self, X):\n","    # Assign data points to closest centroid\n","    distance = np.zeros((X.shape[0], self.n_clusters))\n","    for j in range(self.n_clusters):\n","      distance[:, j] = np.sum((X - self.cluster_centers_[j]) ** 2, axis=1)\n","    \n","    labels = np.argmin(distance, axis=1) # predicted labels\n","    return labels\n","\n","  def fit_predict(self, X):\n","    self.fit(X)\n","    return self.predict(X)\n","\n","  def transform(self, X):\n","    # Transform data to distance matrix (distance matrix contains the Euclidean distances between data points and cluster centers)\n","    distance = np.zeros((X.shape[0], self.n_clusters))\n","    for j in range(self.n_clusters):\n","      distance[:, j] = np.sqrt(np.sum((X - self.cluster_centers_[j]) ** 2, axis=1))\n","    return distance\n","\n","  def fit_transform(self, X):\n","    self.fit(X)\n","    return self.transform(X)\n","\n","  def score(self, X):\n","    # Calculate score (negative inertia)\n","    distance = np.zeros((X.shape[0], self.n_clusters))\n","    for j in range(self.n_clusters):\n","      distance[:, j] = np.sum((X - self.cluster_centers_[j]) ** 2, axis=1)\n","    labels = np.argmin(distance, axis=1) # predicted labels\n","    return -np.sum(distance[np.arange(len(distance)), labels])"],"metadata":{"id":"Evr3jRCT2eDG"},"execution_count":null,"outputs":[]}]}